{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM nvidia/cuda:9.0-base-ubuntu16.04\r\n",
      "\r\n",
      "MAINTAINER Amazon AI\r\n",
      "\r\n",
      "ARG framework_installable=tensorflow-1.9.0-cp27-cp27mu-manylinux1_x86_64.whl\r\n",
      "ARG framework_support_installable=sagemaker_tensorflow_container-1.0.0.tar.gz\r\n",
      "\r\n",
      "RUN apt-get update && apt-get install -y --no-install-recommends \\\r\n",
      "        build-essential \\\r\n",
      "        cuda-command-line-tools-9-0 \\\r\n",
      "        cuda-cublas-dev-9-0 \\\r\n",
      "        cuda-cudart-dev-9-0 \\\r\n",
      "        cuda-cufft-dev-9-0 \\\r\n",
      "        cuda-curand-dev-9-0 \\\r\n",
      "        cuda-cusolver-dev-9-0 \\\r\n",
      "        cuda-cusparse-dev-9-0 \\\r\n",
      "        curl \\\r\n",
      "        git \\\r\n",
      "        libcudnn7=7.1.4.18-1+cuda9.0 \\\r\n",
      "        libcudnn7-dev=7.1.4.18-1+cuda9.0 \\\r\n",
      "        libcurl3-dev \\\r\n",
      "        libfreetype6-dev \\\r\n",
      "        libpng12-dev \\\r\n",
      "        libzmq3-dev \\\r\n",
      "        pkg-config \\\r\n",
      "        python-dev \\\r\n",
      "        rsync \\\r\n",
      "        software-properties-common \\\r\n",
      "        unzip \\\r\n",
      "        zip \\\r\n",
      "        zlib1g-dev \\\r\n",
      "        wget \\\r\n",
      "        vim \\\r\n",
      "        nginx \\\r\n",
      "        iputils-ping \\\r\n",
      "        && \\\r\n",
      "    rm -rf /var/lib/apt/lists/* && \\\r\n",
      "    find /usr/local/cuda-9.0/lib64/ -type f -name 'lib*_static.a' -not -name 'libcudart_static.a' -delete && \\\r\n",
      "    rm /usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n",
      "\r\n",
      "RUN curl -fSsL -O https://bootstrap.pypa.io/get-pip.py && \\\r\n",
      "    python get-pip.py && \\\r\n",
      "    rm get-pip.py\r\n",
      "\r\n",
      "RUN pip --no-cache-dir install \\\r\n",
      "        numpy \\\r\n",
      "        scipy \\\r\n",
      "        sklearn \\\r\n",
      "        pandas \\\r\n",
      "        Pillow \\\r\n",
      "        h5py\r\n",
      "\r\n",
      "# Set up grpc\r\n",
      "RUN pip install enum34 futures mock six && \\\r\n",
      "    pip install --pre 'protobuf>=3.0.0a3' && \\\r\n",
      "    pip install -i https://testpypi.python.org/simple --pre grpcio\r\n",
      "\r\n",
      "# Set up Bazel.\r\n",
      "\r\n",
      "# Running bazel inside a `docker build` command causes trouble, cf:\r\n",
      "#   https://github.com/bazelbuild/bazel/issues/134\r\n",
      "# The easiest solution is to set up a bazelrc file forcing --batch.\r\n",
      "RUN echo \"startup --batch\" >>/etc/bazel.bazelrc\r\n",
      "# Similarly, we need to workaround sandboxing issues:\r\n",
      "#   https://github.com/bazelbuild/bazel/issues/418\r\n",
      "RUN echo \"build --spawn_strategy=standalone --genrule_strategy=standalone\" \\\r\n",
      "    >>/etc/bazel.bazelrc\r\n",
      "# Install the most recent bazel release which works: https://github.com/bazelbuild/bazel/issues/4652\r\n",
      "ENV BAZEL_VERSION 0.10.1\r\n",
      "WORKDIR /\r\n",
      "RUN mkdir /bazel && \\\r\n",
      "    cd /bazel && \\\r\n",
      "    curl -H \"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" -fSsL -O https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \\\r\n",
      "    curl -H \"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" -fSsL -o /bazel/LICENSE.txt https://raw.githubusercontent.com/bazelbuild/bazel/master/LICENSE && \\\r\n",
      "    chmod +x bazel-*.sh && \\\r\n",
      "    ./bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \\\r\n",
      "    cd / && \\\r\n",
      "    rm -f /bazel/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\r\n",
      "\r\n",
      "# Configure the build for our CUDA configuration.\r\n",
      "ENV CI_BUILD_PYTHON python\r\n",
      "ENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n",
      "ENV TF_NEED_CUDA 1\r\n",
      "ENV TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.1\r\n",
      "ENV TF_CUDA_VERSION=9.0\r\n",
      "ENV TF_CUDNN_VERSION=7\r\n",
      "ENV CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu\r\n",
      "\r\n",
      "# TODO: upgrade to tf serving 1.8, which requires more work with updating\r\n",
      "# dependencies. See current work in progress in tfserving-1.8 branch.\r\n",
      "ENV TF_SERVING_VERSION=1.7.0\r\n",
      "\r\n",
      "# Install tensorflow-serving-api\r\n",
      "RUN pip install tensorflow-serving-api==$TF_SERVING_VERSION\r\n",
      "\r\n",
      "# Download TensorFlow Serving\r\n",
      "RUN cd / && git clone --recurse-submodules https://github.com/tensorflow/serving && \\\r\n",
      "  cd serving && \\\r\n",
      "  git checkout $TF_SERVING_VERSION\r\n",
      "\r\n",
      "# Configure Tensorflow to use the GPU\r\n",
      "WORKDIR /serving\r\n",
      "RUN git clone --recursive https://github.com/tensorflow/tensorflow.git && \\\r\n",
      "  cd tensorflow && \\\r\n",
      "  git checkout v$TF_SERVING_VERSION && \\\r\n",
      "  tensorflow/tools/ci_build/builds/configured GPU\r\n",
      "\r\n",
      "# Build TensorFlow Serving and Install it in /usr/local/bin\r\n",
      "WORKDIR /serving\r\n",
      "RUN bazel build -c opt --config=cuda \\\r\n",
      "    --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" \\\r\n",
      "    --crosstool_top=@local_config_cuda//crosstool:toolchain \\\r\n",
      "    tensorflow_serving/model_servers:tensorflow_model_server && \\\r\n",
      "    cp bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server /usr/local/bin/ && \\\r\n",
      "    bazel clean --expunge\r\n",
      "\r\n",
      "# Update libstdc++6, as required by tensorflow-serving >= 1.6: https://github.com/tensorflow/serving/issues/819\r\n",
      "RUN add-apt-repository ppa:ubuntu-toolchain-r/test -y && \\\r\n",
      "    apt-get update && \\\r\n",
      "    apt-get install -y libstdc++6\r\n",
      "\r\n",
      "# cleaning up the container\r\n",
      "RUN rm -rf /serving && \\\r\n",
      "    rm -rf /bazel\r\n",
      "\r\n",
      "WORKDIR /root\r\n",
      "\r\n",
      "# Will install from pypi once packages are released there. For now, copy from local file system.\r\n",
      "COPY $framework_installable .\r\n",
      "COPY $framework_support_installable .\r\n",
      "\r\n",
      "RUN framework_installable_local=$(basename $framework_installable) && \\\r\n",
      "    framework_support_installable_local=$(basename $framework_support_installable) && \\\r\n",
      "    \\\r\n",
      "    pip install --no-cache --upgrade $framework_installable_local && \\\r\n",
      "    pip install $framework_support_installable_local && \\\r\n",
      "    pip install \"sagemaker-tensorflow>=1.9,<1.10\" &&\\\r\n",
      "    \\\r\n",
      "    rm $framework_installable_local && \\\r\n",
      "    rm $framework_support_installable_local\r\n",
      "\r\n",
      "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\r\n",
      "         wget \\\r\n",
      "         ca-certificates \\\r\n",
      "         libgtk2.0-dev \\\r\n",
      "    && rm -rf /var/lib/apt/lists/*\r\n",
      "\r\n",
      "# Here we get all python packages.\r\n",
      "# There's substantial overlap between scipy and numpy that we eliminate by\r\n",
      "# linking them together. Likewise, pip leaves the install caches populated which uses\r\n",
      "# a significant amount of space. These optimizations save a fair amount of space in the\r\n",
      "# image, which reduces start up time.\r\n",
      "\r\n",
      "RUN pip install keras\r\n",
      "RUN pip install configobj\r\n",
      "RUN pip install config\r\n",
      "RUN pip install opencv-python\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\r\n",
      "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\r\n",
      "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\r\n",
      "# PATH so that the train and serve programs are found when the container is invoked.\r\n",
      "\r\n",
      "ENV PYTHONUNBUFFERED=TRUE\r\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\r\n",
      "ENV PATH=\"/opt/program:${PATH}\"\r\n",
      "\r\n",
      "# Set up the program in the image\r\n",
      "COPY keras-test /opt/program\r\n",
      "WORKDIR /opt/program\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and registering the container\n",
    "\n",
    "The following shell code shows how to build the container image using `docker build` and push the container image to ECR using `docker push`. This code is also available as the shell script `container/build-and-push.sh`, which you can run as `build-and-push.sh decision_trees_sample` to build the image `decision_trees_sample`. \n",
    "\n",
    "This code looks for an ECR repository in the account you're using and the current default region (if you're using a SageMaker notebook instance, this will be the region where the notebook instance was created). If the repository doesn't exist, the script will create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  470.9MB\r",
      "\r\n",
      "Step 1/43 : FROM nvidia/cuda:9.0-base-ubuntu16.04\n",
      " ---> 9dcd7cd95db6\n",
      "Step 2/43 : MAINTAINER Amazon AI\n",
      " ---> Using cache\n",
      " ---> 972f6d23a93d\n",
      "Step 3/43 : ARG framework_installable=tensorflow-1.9.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      " ---> Using cache\n",
      " ---> 78aec2c5b4a5\n",
      "Step 4/43 : ARG framework_support_installable=sagemaker_tensorflow_container-1.0.0.tar.gz\n",
      " ---> Using cache\n",
      " ---> e0da941810dd\n",
      "Step 5/43 : RUN apt-get update && apt-get install -y --no-install-recommends         build-essential         cuda-command-line-tools-9-0         cuda-cublas-dev-9-0         cuda-cudart-dev-9-0         cuda-cufft-dev-9-0         cuda-curand-dev-9-0         cuda-cusolver-dev-9-0         cuda-cusparse-dev-9-0         curl         git         libcudnn7=7.1.4.18-1+cuda9.0         libcudnn7-dev=7.1.4.18-1+cuda9.0         libcurl3-dev         libfreetype6-dev         libpng12-dev         libzmq3-dev         pkg-config         python-dev         rsync         software-properties-common         unzip         zip         zlib1g-dev         wget         vim         nginx         iputils-ping         &&     rm -rf /var/lib/apt/lists/* &&     find /usr/local/cuda-9.0/lib64/ -type f -name 'lib*_static.a' -not -name 'libcudart_static.a' -delete &&     rm /usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\n",
      " ---> Using cache\n",
      " ---> fc989fac5180\n",
      "Step 6/43 : RUN curl -fSsL -O https://bootstrap.pypa.io/get-pip.py &&     python get-pip.py &&     rm get-pip.py\n",
      " ---> Using cache\n",
      " ---> bd336cd80d10\n",
      "Step 7/43 : RUN pip --no-cache-dir install         numpy         scipy         sklearn         pandas         Pillow         h5py\n",
      " ---> Using cache\n",
      " ---> 85c56d7307c9\n",
      "Step 8/43 : RUN pip install enum34 futures mock six &&     pip install --pre 'protobuf>=3.0.0a3' &&     pip install -i https://testpypi.python.org/simple --pre grpcio\n",
      " ---> Using cache\n",
      " ---> fb00137069e9\n",
      "Step 9/43 : RUN echo \"startup --batch\" >>/etc/bazel.bazelrc\n",
      " ---> Using cache\n",
      " ---> d83b681610c5\n",
      "Step 10/43 : RUN echo \"build --spawn_strategy=standalone --genrule_strategy=standalone\"     >>/etc/bazel.bazelrc\n",
      " ---> Using cache\n",
      " ---> 72c99f1dd09d\n",
      "Step 11/43 : ENV BAZEL_VERSION 0.10.1\n",
      " ---> Using cache\n",
      " ---> 5f0f4ef2ae7b\n",
      "Step 12/43 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> d84f2e181e4d\n",
      "Step 13/43 : RUN mkdir /bazel &&     cd /bazel &&     curl -H \"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" -fSsL -O https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh &&     curl -H \"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" -fSsL -o /bazel/LICENSE.txt https://raw.githubusercontent.com/bazelbuild/bazel/master/LICENSE &&     chmod +x bazel-*.sh &&     ./bazel-$BAZEL_VERSION-installer-linux-x86_64.sh &&     cd / &&     rm -f /bazel/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\n",
      " ---> Using cache\n",
      " ---> 62113e0f8ac4\n",
      "Step 14/43 : ENV CI_BUILD_PYTHON python\n",
      " ---> Using cache\n",
      " ---> 612cc220e7c0\n",
      "Step 15/43 : ENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\n",
      " ---> Using cache\n",
      " ---> 2a79947a7527\n",
      "Step 16/43 : ENV TF_NEED_CUDA 1\n",
      " ---> Using cache\n",
      " ---> 51876e814bb3\n",
      "Step 17/43 : ENV TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.1\n",
      " ---> Using cache\n",
      " ---> 2b689166b688\n",
      "Step 18/43 : ENV TF_CUDA_VERSION=9.0\n",
      " ---> Using cache\n",
      " ---> d5ead6d3fe1c\n",
      "Step 19/43 : ENV TF_CUDNN_VERSION=7\n",
      " ---> Using cache\n",
      " ---> c65eb9e2aafa\n",
      "Step 20/43 : ENV CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu\n",
      " ---> Using cache\n",
      " ---> 09ee3ff39605\n",
      "Step 21/43 : ENV TF_SERVING_VERSION=1.7.0\n",
      " ---> Using cache\n",
      " ---> 06752de43ae1\n",
      "Step 22/43 : RUN pip install tensorflow-serving-api==$TF_SERVING_VERSION\n",
      " ---> Using cache\n",
      " ---> a171c632f6ad\n",
      "Step 23/43 : RUN cd / && git clone --recurse-submodules https://github.com/tensorflow/serving &&   cd serving &&   git checkout $TF_SERVING_VERSION\n",
      " ---> Using cache\n",
      " ---> a614c94c95f8\n",
      "Step 24/43 : WORKDIR /serving\n",
      " ---> Using cache\n",
      " ---> da68beb4396f\n",
      "Step 25/43 : RUN git clone --recursive https://github.com/tensorflow/tensorflow.git &&   cd tensorflow &&   git checkout v$TF_SERVING_VERSION &&   tensorflow/tools/ci_build/builds/configured GPU\n",
      " ---> Using cache\n",
      " ---> cbd98eaa4bac\n",
      "Step 26/43 : WORKDIR /serving\n",
      " ---> Using cache\n",
      " ---> 86e3e4daf32a\n",
      "Step 27/43 : RUN bazel build -c opt --config=cuda     --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"     --crosstool_top=@local_config_cuda//crosstool:toolchain     tensorflow_serving/model_servers:tensorflow_model_server &&     cp bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server /usr/local/bin/ &&     bazel clean --expunge\n",
      " ---> Using cache\n",
      " ---> 7076b66726d8\n",
      "Step 28/43 : RUN add-apt-repository ppa:ubuntu-toolchain-r/test -y &&     apt-get update &&     apt-get install -y libstdc++6\n",
      " ---> Using cache\n",
      " ---> 15622390844e\n",
      "Step 29/43 : RUN rm -rf /serving &&     rm -rf /bazel\n",
      " ---> Using cache\n",
      " ---> 8f7567a0d402\n",
      "Step 30/43 : WORKDIR /root\n",
      " ---> Using cache\n",
      " ---> 8f79ada5a355\n",
      "Step 31/43 : COPY $framework_installable .\n",
      " ---> Using cache\n",
      " ---> 135bf4297ce4\n",
      "Step 32/43 : COPY $framework_support_installable .\n",
      " ---> Using cache\n",
      " ---> 55a0249a1be2\n",
      "Step 33/43 : RUN framework_installable_local=$(basename $framework_installable) &&     framework_support_installable_local=$(basename $framework_support_installable) &&         pip install --no-cache --upgrade $framework_installable_local &&     pip install $framework_support_installable_local &&     pip install \"sagemaker-tensorflow>=1.9,<1.10\" &&        rm $framework_installable_local &&     rm $framework_support_installable_local\n",
      " ---> Using cache\n",
      " ---> ce08b6407374\n",
      "Step 34/43 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          ca-certificates          libgtk2.0-dev     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 55ff5c743cde\n",
      "Step 35/43 : RUN pip install keras\n",
      " ---> Using cache\n",
      " ---> 96452677e391\n",
      "Step 36/43 : RUN pip install configobj\n",
      " ---> Using cache\n",
      " ---> d5d6a1c2033b\n",
      "Step 37/43 : RUN pip install config\n",
      " ---> Using cache\n",
      " ---> 3886a34ac736\n",
      "Step 38/43 : RUN pip install opencv-python\n",
      " ---> Using cache\n",
      " ---> 0ca1e0fe3433\n",
      "Step 39/43 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 4a1b4f9b4e82\n",
      "Step 40/43 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 0b939fe86861\n",
      "Step 41/43 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> d3d8f42adad4\n",
      "Step 42/43 : COPY keras-test /opt/program\n",
      " ---> e142adb5c43d\n",
      "Step 43/43 : WORKDIR /opt/program\n",
      " ---> Running in 51a20cdcbc33\n",
      "Removing intermediate container 51a20cdcbc33\n",
      " ---> f71bd63d38b7\n",
      "Successfully built f71bd63d38b7\n",
      "Successfully tagged keras-test:latest\n",
      "The push refers to repository [452432741922.dkr.ecr.us-east-1.amazonaws.com/keras-test]\n",
      "3c69f43d31a0: Preparing\n",
      "3f3c132c3438: Preparing\n",
      "d87942cf9012: Preparing\n",
      "15e119707274: Preparing\n",
      "6b03a598744a: Preparing\n",
      "e51c673c4621: Preparing\n",
      "fefccb201a7a: Preparing\n",
      "bc0b42978038: Preparing\n",
      "3a3d186792ab: Preparing\n",
      "4723f7f00729: Preparing\n",
      "a24cfbe99f42: Preparing\n",
      "c10497a6e110: Preparing\n",
      "4d1f79ec0735: Preparing\n",
      "44e5991db4c4: Preparing\n",
      "6cef9afb9ce2: Preparing\n",
      "56e9f1339377: Preparing\n",
      "6e62f0ea9b01: Preparing\n",
      "f255469cbbe5: Preparing\n",
      "9cc10ae815c2: Preparing\n",
      "e8a9d80f210d: Preparing\n",
      "159c1255ec95: Preparing\n",
      "48f01cc9c497: Preparing\n",
      "cadf36f9989b: Preparing\n",
      "03da9992e0d0: Preparing\n",
      "0b3aca7c75ea: Preparing\n",
      "297fd071ca2f: Preparing\n",
      "2f0d1e8214b2: Preparing\n",
      "7dd604ffa87f: Preparing\n",
      "aa54c2bc1229: Preparing\n",
      "c10497a6e110: Waiting\n",
      "4d1f79ec0735: Waiting\n",
      "44e5991db4c4: Waiting\n",
      "6cef9afb9ce2: Waiting\n",
      "56e9f1339377: Waiting\n",
      "6e62f0ea9b01: Waiting\n",
      "e51c673c4621: Waiting\n",
      "f255469cbbe5: Waiting\n",
      "9cc10ae815c2: Waiting\n",
      "e8a9d80f210d: Waiting\n",
      "159c1255ec95: Waiting\n",
      "48f01cc9c497: Waiting\n",
      "3a3d186792ab: Waiting\n",
      "fefccb201a7a: Waiting\n",
      "cadf36f9989b: Waiting\n",
      "bc0b42978038: Waiting\n",
      "03da9992e0d0: Waiting\n",
      "0b3aca7c75ea: Waiting\n",
      "4723f7f00729: Waiting\n",
      "a24cfbe99f42: Waiting\n",
      "7dd604ffa87f: Waiting\n",
      "3f3c132c3438: Layer already exists\n",
      "15e119707274: Layer already exists\n",
      "6b03a598744a: Layer already exists\n",
      "d87942cf9012: Layer already exists\n",
      "bc0b42978038: Layer already exists\n",
      "e51c673c4621: Layer already exists\n",
      "fefccb201a7a: Layer already exists\n",
      "3a3d186792ab: Layer already exists\n",
      "c10497a6e110: Layer already exists\n",
      "a24cfbe99f42: Layer already exists\n",
      "4723f7f00729: Layer already exists\n",
      "4d1f79ec0735: Layer already exists\n",
      "6cef9afb9ce2: Layer already exists\n",
      "6e62f0ea9b01: Layer already exists\n",
      "44e5991db4c4: Layer already exists\n",
      "56e9f1339377: Layer already exists\n",
      "3c69f43d31a0: Pushed\n",
      "e8a9d80f210d: Layer already exists\n",
      "159c1255ec95: Layer already exists\n",
      "f255469cbbe5: Layer already exists\n",
      "9cc10ae815c2: Layer already exists\n",
      "0b3aca7c75ea: Layer already exists\n",
      "48f01cc9c497: Layer already exists\n",
      "03da9992e0d0: Layer already exists\n",
      "297fd071ca2f: Layer already exists\n",
      "cadf36f9989b: Layer already exists\n",
      "7dd604ffa87f: Layer already exists\n",
      "aa54c2bc1229: Layer already exists\n",
      "2f0d1e8214b2: Layer already exists\n",
      "latest: digest: sha256:addbf3570189ed1f2fed0f4b6baeb5cdec46699a3bbac42dde9fd8594865f88f size: 6417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=keras-test\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x keras-test/train\n",
    "chmod +x keras-test/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your algorithm on your local machine or on an Amazon SageMaker notebook instance\n",
    "\n",
    "While you're first packaging an algorithm use with Amazon SageMaker, you probably want to test it yourself to make sure it's working right. In the directory `container/local_test`, there is a framework for doing this. It includes three shell scripts for running and using the container and a directory structure that mimics the one outlined above.\n",
    "\n",
    "The scripts are:\n",
    "\n",
    "* `train_local.sh`: Run this with the name of the image and it will run training on the local tree. You'll want to modify the directory `test_dir/input/data/...` to be set up with the correct channels and data for your algorithm. Also, you'll want to modify the file `input/config/hyperparameters.json` to have the hyperparameter settings that you want to test (as strings).\n",
    "* `serve_local.sh`: Run this with the name of the image once you've trained the model and it should serve the model. It will run and wait for requests. Simply use the keyboard interrupt to stop it.\n",
    "* `predict.sh`: Run this with the name of a payload file and (optionally) the HTTP content type you want. The content type will default to `text/csv`. For example, you can run `$ ./predict.sh payload.csv text/csv`.\n",
    "\n",
    "The directories as shipped are set up to test the decision trees sample algorithm presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training and Hosting your Algorithm in Amazon SageMaker\n",
    "\n",
    "Once you have your container packaged, you can use it to train and serve models. Let's do that with the algorithm we made above.\n",
    "\n",
    "## Set up the environment\n",
    "\n",
    "Here we specify a bucket to use and the role that will be used for working with SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = 'boxing'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session\n",
    "\n",
    "The session remembers our connection parameters to SageMaker. We'll use it to perform all of our SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. For the purposes of this example, we're using some the classic [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), which we have included. \n",
    "\n",
    "We can use use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-452432741922\n"
     ]
    }
   ],
   "source": [
    "WORK_DIRECTORY = 'data'\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an estimator and fit the model\n",
    "\n",
    "In order to use SageMaker to fit our algorithm, we'll create an `Estimator` that defines how to use the container to train. This includes the configuration we need to invoke SageMaker training:\n",
    "\n",
    "* The __container name__. This is constructed as in the shell commands above.\n",
    "* The __role__. As defined above.\n",
    "* The __instance count__ which is the number of machines to use for training.\n",
    "* The __instance type__ which is the type of machine to use for training.\n",
    "* The __output path__ determines where the model artifact will be written.\n",
    "* The __session__ is the SageMaker session object that we defined above.\n",
    "\n",
    "Then we use fit() on the estimator to train against the data that we uploaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: keras-test-2019-04-10-15-02-32-597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-10 15:02:32 Starting - Starting the training job...\n",
      "2019-04-10 15:02:35 Starting - Launching requested ML instances.........\n",
      "2019-04-10 15:04:16 Starting - Preparing the instances for training............\n",
      "2019-04-10 15:06:29 Downloading - Downloading input data\n",
      "2019-04-10 15:06:29 Training - Downloading the training image.........\n",
      "2019-04-10 15:07:48 Uploading - Uploading generated training model...\n",
      "2019-04-10 15:08:24 Completed - Training job completed\n",
      "..\n",
      "Billable seconds: 123\n"
     ]
    }
   ],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/keras-test:latest'.format(account, region)\n",
    "\n",
    "model = sage.estimator.Estimator(image,\n",
    "                       role, 1, 'ml.p2.16xlarge',\n",
    "                       output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                       sagemaker_session=sess)\n",
    "\n",
    "model.fit(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'452432741922'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account = '452432741922'\n",
    "\n",
    "region = 'us-east-1'\n",
    "\n",
    "image = '452432741922.dkr.ecr.us-east-1.amazonaws.com/keras-test:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "Deploying the model to SageMaker hosting just requires a `deploy` call on the fitted model. This call takes an instance count, instance type, and optionally serializer and deserializer functions. These are used when the resulting predictor is created on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: keras-test-2019-04-10-15-08-48-782\n",
      "INFO:sagemaker:Creating endpoint with name keras-test-2019-04-10-15-02-32-597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = model.deploy(1, 'ml.p3.8xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import cv2\n",
    "img = cv2.imread('test.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, img_encoded = cv2.imencode('.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_type = 'image/jpeg'\n",
    "headers = {'content-type': content_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "endpoint_name = \"keras-test-2019-04-05-16-27-28-388\"                                       # Your endpoint name.\n",
    "content_type = \"image/jpeg\"                                        # The MIME type of the input data in the request body.\n",
    "payload = img_encoded.tostring()                                             # Payload for inference.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=content_type,\n",
    "    Body=payload\n",
    "    )\n",
    "\n",
    "print(response)                         # If model receives and updates the custom_attributes header \n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['Body'].read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional cleanup\n",
    "\n",
    "When you're done with the endpoint, you'll want to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
